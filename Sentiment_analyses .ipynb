{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #tokenize word\n",
    "from nltk.stem import WordNetLemmatizer #run ran running => make same actual word\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle #save file\n",
    "from collections import Counter #count word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lexicon(pos,neg):\n",
    "\n",
    "    lexicon = []\n",
    "    with open(pos,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "\n",
    "    with open(neg,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_handling(sample,lexicon,classification):\n",
    "\n",
    "    featureset = []\n",
    "\n",
    "    with open(sample,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(l.lower())\n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    features[index_value] += 1\n",
    "\n",
    "            features = list(features)\n",
    "            featureset.append([features,classification])\n",
    "\n",
    "    return featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n",
    "    lexicon = create_lexicon(pos,neg)\n",
    "    features = []\n",
    "    features += sample_handling('pos.txt',lexicon,[1,0])\n",
    "    random.shuffle(features)\n",
    "    features += sample_handling('neg.txt',lexicon,[0,1])\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "\n",
    "    testing_size = int(test_size*len(features))\n",
    "\n",
    "    train_x = list(features[:,0][:-testing_size])\n",
    "    train_y = list(features[:,1][:-testing_size])\n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    test_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "\n",
    "    with open('sentiment_set.pickle','wb') as f:\n",
    "        pickle.dump([train_x,train_y,test_x,test_y],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 1500\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "batch_size = 100\n",
    "hm_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float')\n",
    "Y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=Y) )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    n_epochs = 30\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer() )\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            i=0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                              Y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "            \n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss,\n",
    "                  'With Accuracy:',accuracy.eval({x:test_x,Y:test_y}))\n",
    "\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('\\n\\n','#'*50,'\\n Total Accuracy:',accuracy.eval({x:test_x,Y:test_y}),'\\n','#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 10 loss: 1253146.99121 With Accuracy: 0.589118\n",
      "Epoch 2 completed out of 10 loss: 537607.225586 With Accuracy: 0.596623\n",
      "Epoch 3 completed out of 10 loss: 428961.930908 With Accuracy: 0.618199\n",
      "Epoch 4 completed out of 10 loss: 429376.342896 With Accuracy: 0.625704\n",
      "Epoch 5 completed out of 10 loss: 162862.384064 With Accuracy: 0.582552\n",
      "Epoch 6 completed out of 10 loss: 129290.723236 With Accuracy: 0.625704\n",
      "Epoch 7 completed out of 10 loss: 150139.01918 With Accuracy: 0.58818\n",
      "Epoch 8 completed out of 10 loss: 175318.36879 With Accuracy: 0.61257\n",
      "Epoch 9 completed out of 10 loss: 48992.6575661 With Accuracy: 0.630394\n",
      "Epoch 10 completed out of 10 loss: 13355.5717468 With Accuracy: 0.620075\n",
      "Epoch 11 completed out of 10 loss: 11651.3329475 With Accuracy: 0.614447\n",
      "Epoch 12 completed out of 10 loss: 8369.30546999 With Accuracy: 0.615385\n",
      "Epoch 13 completed out of 10 loss: 9381.27617818 With Accuracy: 0.626642\n",
      "Epoch 14 completed out of 10 loss: 9262.21038318 With Accuracy: 0.621951\n",
      "Epoch 15 completed out of 10 loss: 8834.49364471 With Accuracy: 0.614447\n",
      "Epoch 16 completed out of 10 loss: 11263.2433248 With Accuracy: 0.617261\n",
      "Epoch 17 completed out of 10 loss: 9434.59066413 With Accuracy: 0.616323\n",
      "Epoch 18 completed out of 10 loss: 11538.5959897 With Accuracy: 0.610694\n",
      "Epoch 19 completed out of 10 loss: 11490.052471 With Accuracy: 0.61257\n",
      "Epoch 20 completed out of 10 loss: 10499.7353415 With Accuracy: 0.613508\n",
      "Epoch 21 completed out of 10 loss: 11563.0079415 With Accuracy: 0.611632\n",
      "Epoch 22 completed out of 10 loss: 12882.5132341 With Accuracy: 0.617261\n",
      "Epoch 23 completed out of 10 loss: 12104.9549628 With Accuracy: 0.617261\n",
      "Epoch 24 completed out of 10 loss: 15411.566581 With Accuracy: 0.620075\n",
      "Epoch 25 completed out of 10 loss: 14336.648757 With Accuracy: 0.614447\n",
      "Epoch 26 completed out of 10 loss: 15392.9169731 With Accuracy: 0.610694\n",
      "Epoch 27 completed out of 10 loss: 14502.4754565 With Accuracy: 0.604128\n",
      "Epoch 28 completed out of 10 loss: 15085.6505156 With Accuracy: 0.611632\n",
      "Epoch 29 completed out of 10 loss: 15040.0258152 With Accuracy: 0.610694\n",
      "Epoch 30 completed out of 10 loss: 16016.2244935 With Accuracy: 0.609756\n",
      "\n",
      "\n",
      " ################################################## \n",
      " Total Accuracy: 0.609756 \n",
      " ##################################################\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
